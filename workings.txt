===============================

data1:
    binary | ones    | correct?
    00     | # # # 1 | 1
    01     | # # 1 # | 1
    10     | # 1 # # | 1
    11     | 1 # # # | 1
    ##     | # # # # | 0
    ^          ^       ^
    |__________|       |
    rule               action

    00 = 4th bit
    01 = 3rd bit
    10 = 2nd bit
    11 = 1st bit

    (negative example with bounds)
    c/logs/data1/2019-11-19.19:12:08.log

===============================

===============================

data2:
    Solution:
        110110 0
        011101 0
        00#110 0
        011010 1
        001000 1
        010100 0
        #111#1 1
        111000 1
        #0100# 0
        0#1#01 1
        110011 0
        100100 0
        ##1#01 0
        00#0#1 1
        11#0#1 1
        #110## 0
        ##01#0 1
        11##1# 1
        00##0# 0
        0#1#1# 0
        ##0011 1
        ###110 0
        ###0#1 0
        0###0# 1
        #1#### 0
        ####0# 1
        1#0### 0
        ###### 1

    Missing Value Predictions:
        Attributes: 000001 Predicted Label: 1
        Attributes: 001110 Predicted Label: 0
        Attributes: 101001 Predicted Label: 0
        Attributes: 111111 Predicted Label: 1

    Achieved 60/60 (1.0) on training data

    Found 8 hardcoded rules:
        110110 0
        011101 0
        011010 1
        001000 1
        010100 0
        111000 1
        110011 0
        100100 0

    Rule based solution is too complex to understand, and took a very long time
    to converge, there must be a better way.

    Learning from this dataset using a standard dense neural network seemed not
    to fit very quickly either. Even with a large parameter space.

    To try and get any function approximator to at least fit this dataset seemed
    quite difficult.

    So I decided to throw an even larger parameter space at the problem.
    Using an LSTM enabled the network to fit quickly.
    A simple recurrent neural network also achieved the same result.

    This told me there is merit to representing features in the dataset as a
    sequence.

    Simply using a neural network with many parameters is not going to make the
    underlying structure of the data any more clear.

    I implemented a genetic programming model to learn an equation that best
    fit the dataset. It achieved a solution with 98.34% accuracy within 6272
    generations.

    Note: In the real world, data can be messy. Achieving 100% accuracy on the
    training dataset often means the model has overfit the data, and may not
    transfer well to unseen data. Getting below 100% here is fine.

    The solution:
    (((((f5 + f1) + ((f0 + f2) + (f4 + f3))) % ((f5 + ((-0.842402089280506 + f0) + -0.9056793454327423)) + ((f4 + (f1 + -0.916229145372748)) + (f3 + f2)))) %
        ((f5 + ((f1 + f3) + (f2 + f0))) + (((f2 + f1) + f3) + ((f5 % 0.4679532670422235) + (-0.9211416668249195 + f0))))) +
        ((((((f0 % f2) + (f0 + f0)) + (f0 + f4)) + (f5 + f0)) +
        (((f0 % f2) + f2) + ((f2 + f4) + f0))) % f2)) < 0.41920992757675224

    This is of course something quite difficult to pick apart.

    To simplify, I added some constraints to the tree based solution:
    * only use addition and modulo operators
    * constants must be integers
    * reduce reuse of features
    * reduce rate of adding constants

    The optimiser was also modified to more easily enforce the limits supplied.
    Crossover was a difficult feature to support here, so it was removed and the
    mutation rate was increased to keep the variance of solutions high. Somewhat
    loosely representing an artificial immune system. This solved the problem at
    98.34% again, but with much more simple solutions.

    The solutions:
        1st run
        30% mutation rate, 1366 generations
        (((((f1 + f2) + (f3 + f0)) + ((f4 + f4) + (-1 + f4))) + f5) % 2) < 0.44282579177624615

        2nd run (reduced mutation rate due to low mean)
        5% mutation rate, 604 generations
        ((((3 + f1) + ((f0 + f4) + (f3 + f5))) + f2) % 2) < 0.6684807048908901

    Interpreting these solutions, and cross referencing them with the dataset
    looks like:

        prediction = 0 if sum(features) % 2 == 0 else 1

    Which means, if there are an even number of 1s in the features, the label is
    0. Otherwise the label is 1.

    It would seem the genetic programming implementation here is still prone to
    overfitting the data, creating obscure additional logic. The equation:
    `((f4 + f4) + (-1 + f4))` looks as though it could be reduced to `f4`.
    Adding an automated way reward individuals with reduced complexity may be
    helpful here in the future to solve more complex problems. This could be
    achieved by adjusting the fitness function as follows:

    fitness(tree) = (correct_count(tree, dataset) + (size(tree) / max_tree_size^-1)) / size(dataset)

    Of course this solution still does not account for a portion of the dataset,
    specifically 1 value: `000011 1`. This may just be due to it being bad data.

===============================

===============================

data3:
    from solutions_6
    dataset:datasets/2019/data3.txt rule_count:6 generation:503 fitness:1.0 time:2019-11-04_18:44:09.072323 rules:0,1,#,0,#,#,0,1,0,#,#,1,#,1,#,1,#,#,#,1,1,0,1,#,#,#,#,1,0,#,1,#,#,#,1,#,#,#,#,#,#,0

    01#0## 0
    10##1# 1
    #1###1 1  < 01###1 + 11###1 = 1
    01#### 1  < 01###1
    0#1### 1
    ###### 0

    01#0## 0
    10##1# 1
    11###1 1
    01#### 1  < covers   01####
    0#1### 1  < might be 001### 1  ???
    ###### 0

    01 #0## 0  < remove this
    10 ##1# 1
    11 ###1 1
    01 #### 1  < replace # with 1 in position 2
    00 1### 1
    ## #### 0

    00 1### 1
    01 #1## 1
    10 ##1# 1
    11 ###1 1
    ## #### 0

    00 = 1st bit
    01 = 2nd bit
    10 = 3rd bit
    11 = 4th bit

    aka:
    binary | 0 1 2 3 | correct?
    00     | 1 # # # | 1
    01     | # 1 # # | 1
    10     | # # 1 # | 1
    11     | # # # 1 | 1
    ##     | # # # # | 0
    ^                   ^ ^
    |___________________| |
    rule                  action

    c/logs/data3/2019-11-19.20:45:13.log
    rule_count:5 generation:71046 time:2019-11-19.21:21:43 train_fitness_best:0.982 train_fitness_mean:0.632947 train_fitness_first_quartile:0.553667 train_fitness_median:0.698667 train_fitness_third_quartile:0.808667 cross_validation_fitness_best:0.948667 cross_validation_fitness_mean:0.617097 cross_validation_fitness_first_quartile:0.555 cross_validation_fitness_median:0.673667 cross_validation_fitness_third_quartile:0.773667 test_fitness_best:0.939 test_fitness_mean:0.607572 test_fitness_first_quartile:0.545 test_fitness_median:0.65525 test_fitness_third_quartile:0.76525 best_train_rules:-0.010833~0.500701,-0.085301~0.526811,-0.245367~0.502400,#,#,#,0|0.491358~1.041304,0.492993~0.989952,#,#,-0.184474~1.089337,0.504995~1.090061,1|#,-0.214592~0.502411,#,-0.026244~0.987326,0.500973~1.060664,#,1|-0.127040~0.495538,#,#,0.497231~1.076245,#,#,1|#,#,#,#,#,#,0 best_cross_validation_rules:-0.010833~0.500701,-0.085301~0.526811,-0.245367~0.502400,#,#,#,0|0.491358~1.041304,0.492993~0.989952,#,#,-0.184474~1.089337,0.504995~1.090061,1|#,-0.214592~0.502411,#,-0.026244~0.987326,0.500973~1.060664,#,1|-0.127040~0.495538,#,#,0.497231~1.076245,#,#,1|#,#,#,#,#,#,0 best_test_rules:-0.010833~0.500701,-0.085301~0.526811,-0.245367~0.502400,#,#,#,0|0.491358~1.041304,0.492993~0.989952,#,#,-0.184474~1.089337,0.504995~1.090061,1|#,-0.214592~0.502411,#,-0.026244~0.987326,0.500973~1.060664,#,1|-0.127040~0.495538,#,#,0.497231~1.076245,#,#,1|#,#,#,#,#,#,0

===============================

data4:
    something like this i guess? :)
    000 #######
    001 #######
    010 #######
    011 #######
    100 #######
    101 #######
    110 #######
    111 #######

    c/logs/digital_data4/2019-11-19.21:26:10.log
    dataset:../../datasets/2019/digital_data4.txt rule_count:6 generation:425 fitness:1.0002 time:2019-11-19.21:27:10 rules:#,#,-0.092394~0.202581,0.096440~1.210331,#,#,#,#,#,-0.140382~0.320452,1|#,#,#,#,-0.019404~1.056449,-0.223762~0.957631,#,-0.039291~1.110178,#,-0.231433~0.192356,0|#,#,0.860605~1.080023,#,#,#,#,-0.239216~0.141168,#,0.779855~1.160643,0|#,#,0.970951~1.144137,#,#,#,#,#,-0.134057~1.104450,#,1|#,-0.046104~1.120503,#,#,#,#,0.901261~1.033710,#,-0.134057~1.104450,0.187995~1.088149,1|#,#,#,#,#,#,#,#,#,#,0

    #,#,-0.092394~0.202581,0.096440~1.210331,#,#,#,#,#,-0.140382~0.320452,1
    #,#,#,#,-0.019404~1.056449,-0.223762~0.957631,#,-0.039291~1.110178,#,-0.231433~0.192356,0
    #,#,0.860605~1.080023,#,#,#,#,-0.239216~0.141168,#,0.779855~1.160643,0
    #,#,0.970951~1.144137,#,#,#,#,#,-0.134057~1.104450,#,1
    #,-0.046104~1.120503,#,#,#,#,0.901261~1.033710,#,-0.134057~1.104450,0.187995~1.088149,1
    #,#,#,#,#,#,#,#,#,#,0

    #,#,0,1,#,#,#,#,#,0,1
    #,#,#,#,#,0,#,#,#,0,0
    #,#,1,#,#,#,#,0,#,1,0
    #,#,1,#,#,#,#,#,#,#,1
    #,#,#,#,#,#,1,#,#,1,1
    #,#,#,#,#,#,#,#,#,#,0

    ##01#####0 1
    #####0###0 0
    ##1####0#1 0
    ##1####### 1
    ######1##1 1
    ########## 0

    c/logs/data4/2019-11-19.21:31:14.log
    dataset:../../datasets/2019/data4.txt rule_count:6 generation:6227 fitness:0.991441 time:2019-11-19.21:47:25 rules:#,#,-0.200063~0.497454,0.499582~1.094465,#,#,#,#,#,-0.100505~0.502208,1|#,#,0.510111~1.156945,#,#,0.499882~0.991970,#,#,#,-0.048485~0.502208,1|#,#,0.498844~1.003318,#,#,#,#,-0.029772~0.491909,#,0.429699~1.012523,0|#,#,0.510987~1.111334,#,#,#,-0.061163~0.752449,#,#,0.510357~1.249300,1|#,#,#,#,#,#,0.506692~1.244266,#,#,0.488645~1.242908,1|#,#,#,#,#,#,#,#,-0.012584~1.072424,#,0

    #,#,-0.200063~0.497454,0.499582~1.094465,#,#,#,#,#,-0.100505~0.502208,1
    #,#,0.510111~1.156945,#,#,0.499882~0.991970,#,#,#,-0.048485~0.502208,1
    #,#,0.498844~1.003318,#,#,#,#,-0.029772~0.491909,#,0.429699~1.012523,0
    #,#,0.510987~1.111334,#,#,#,-0.061163~0.752449,#,#,0.510357~1.249300,1
    #,#,#,#,#,#,0.506692~1.244266,#,#,0.488645~1.242908,1
    #,#,#,#,#,#,#,#,-0.012584~1.072424,#,0

    #,#,0,1,#,#,#,#,#,0,1
    #,#,1,#,#,1,#,#,#,0,1
    #,#,1,#,#,#,#,0,#,1,0
    #,#,1,#,#,#,#,#,#,1,1
    #,#,#,#,#,#,1,#,#,1,1
    #,#,#,#,#,#,#,#,#,#,0

    ##01#####0 1
    ##1##1###0 1
    ##1####0#1 0
    ##1######1 1
    ######1##1 1
    ########## 0

    ##01#####0 1
    ##1##1###0 1
    ##1####0#1 0  < bubble down 1
    ##1######1 1  < set 1 in 8th position
    ######1##1 1
    ########## 0

    ##01#####0 1
    ##1##1###0 1
    ##1####1#1 1
    ##1####0#1 0  < bubble down 1
    ######1##1 1  < set 0 in 3rd position
    ########## 0

    ##01#####0 1
    ##1##1###0 1
    ##1####1#1 1
    ##0###1##1 1
    ##1####0#1 0  < delete
    ########## 0

    ##01#####0 1
    ##1##1###0 1
    ##1####1#1 1
    ##0###1##1 1
    ########## 0

    sorted - VI ? IV? data4?

    ##1####1#1 1
    ##0###1##1 1
    ##1##1###0 1
    ##01#####0 1
    ########## 0

    place value based on borders (positions 3 and 10)?

    ##01#####0 1
    ##0###1##1 1
    ##1##1###0 1
    ##1####1#1 1
    ########## 0

    un-doing questions larry (possibly) thought:

        here's what we've got:

        a b c d e f g h i j
        # # 0 1 # # # # # 0     1
        # # 1 # # 1 # # # 0     1
        # # 0 # # # 1 # # 1     1
        # # 1 # # # # 1 # 1     1
        # # # # # # # # # #     0

        how about i add a few dummy columns to throw people off?
        (remove columns a, b, e, i)

        c d f g h j
        0 1 # # # 0     1
        1 # 1 # # 0     1
        0 # # 1 # 1     1
        1 # # # 1 1     1
        # # # # # #     0

        how about i reorder the output columns to throw more people off?
        (move j to the front)

        j c d f g h
        0 0 1 # # #     1
        0 1 # 1 # #     1
        1 0 # # 1 #     1
        1 1 # # # 1     1
        # # # # # #     0

===============================
